{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import gedi_adapter\n",
    "reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import modeling_utils\\nimport text_processing\\nimport modeling_gpt2\\nimport gedi_training'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import modeling_utils\n",
    "import text_processing\n",
    "import modeling_gpt2\n",
    "import gedi_training'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: requests in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shuba\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\shuba\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "t5name = 'SkolkovoInstitute/t5-paraphrase-paws-msrp-opinosis-paranmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../transfer_utils/'))\n",
    "\n",
    "import text_processing\n",
    "reload(text_processing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(t5name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = AutoModelForSeq2SeqLM.from_pretrained(t5name)\n",
    "para.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/gpt2-base-gedi-detoxification were not used when initializing GPT2LMHeadModel: ['bias', 'logit_scale']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_path = 'SkolkovoInstitute/gpt2-base-gedi-detoxification'\n",
    "gedi_dis = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_POS = tokenizer.encode('normal', add_special_tokens=False)[0]\n",
    "NEW_NEG = tokenizer.encode('toxic', add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0844, -0.0844]]) tensor([[1.2702]])\n"
     ]
    }
   ],
   "source": [
    "# add gedi-specific parameters\n",
    "if os.path.exists(model_path):\n",
    "    w = torch.load(model_path + '/pytorch_model.bin', map_location='cpu')\n",
    "    gedi_dis.bias = w['bias']\n",
    "    gedi_dis.logit_scale = w['logit_scale']\n",
    "    del w\n",
    "else:\n",
    "    gedi_dis.bias = torch.tensor([[ 0.08441592, -0.08441573]])\n",
    "    gedi_dis.logit_scale = torch.tensor([[1.2701858]])\n",
    "print(gedi_dis.bias, gedi_dis.logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEFORE====\n",
      "The internal policy of Trump is flawed.\n",
      "====AFTER=====\n",
      "hey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuba\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president's a corruption to Iran and Iraq is corrupt.\n",
      "the trumps is fooling themselves with his policies!\n",
      "the president's head is wrong in his own personal sphere; not to mention, his\n",
      "CPU times: total: 16.2 s\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gedi_adapter import GediAdapter\n",
    "import transformers\n",
    "#import modeling_utils\n",
    "dadapter = GediAdapter(model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=5, target=1, neg_code=NEW_NEG, pos_code=NEW_POS, lb=None, ub=None)\n",
    "text = 'The internal policy of Trump is flawed.'\n",
    "print('====BEFORE====')\n",
    "print(text)\n",
    "print('====AFTER=====')\n",
    "#input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to(para.device)\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(para.device)\n",
    "print(\"hey\")\n",
    "result=dadapter.generate(inputs, num_return_sequences=3, do_sample=True, temperature=1.0, repetition_penalty=3.0,num_beams=1)\n",
    "for r in result:\n",
    "    print(tokenizer.decode(r, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available() and device.type != 'cpu':\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "para.to(device);\n",
    "para.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gedi_dis.to(device);\n",
    "gedi_dis.bias = gedi_dis.bias.to(device)\n",
    "gedi_dis.logit_scale = gedi_dis.logit_scale.to(device)\n",
    "gedi_dis.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/test/test_10k_toxic', 'r') as f:\n",
    "    test_data = [line.strip() for line in f.readlines()]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadapter = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=10, target=0, neg_code=NEW_NEG, pos_code=NEW_POS, \n",
    "    reg_alpha=3e-5, ub=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, n=None, max_length='auto', beams=2):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(dadapter.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    result = dadapter.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, temperature=0.0, repetition_penalty=3.0, max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    texts = [text_processing.text_postprocess(t) for t in texts]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You'd be a bad guy. I mean, you'd have to do\",\n",
       " \"'Lose One', as good and bad as the boss.\",\n",
       " 'A bad society does bad things, and votes for bad politicians.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(test_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "clf_name = 'SkolkovoInstitute/roberta_toxicity_classifier_v1'\n",
    "clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device);\n",
    "clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(texts):\n",
    "    with torch.inference_mode():\n",
    "        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n",
    "        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.0525083e-05, 8.7885252e-05, 9.9968088e-01], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "predict_toxicity(['hello world', 'hello aussie', 'hello fucking bitch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This article's bad.\",\n",
       " \"Bits I've warned you.\",\n",
       " 'I was an ad.',\n",
       " \"it'll be a very strange name.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter\n",
    "\n",
    "\n",
    "adapter2 = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, \n",
    "    gedi_logit_coef=10, \n",
    "    target=0, pos_code=NEW_POS, \n",
    "    neg_code=NEW_NEG,\n",
    "    reg_alpha=3e-5,\n",
    "    ub=0.01,\n",
    "    untouchable_tokens=[0, 1],\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(text, max_length='auto', beams=5, rerank=True):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(adapter2.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    attempts = beams\n",
    "    out = adapter2.generate(\n",
    "        inputs, \n",
    "        num_beams=beams,\n",
    "        num_return_sequences=attempts, \n",
    "        do_sample=False, \n",
    "        temperature=1.0, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    results = [tokenizer.decode(r, skip_special_tokens=True) for r in out.sequences]\n",
    "\n",
    "    if rerank:\n",
    "        scores = predict_toxicity(results)\n",
    "    \n",
    "    results = [text_processing.text_postprocess(t) for t in results]\n",
    "    out_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        if rerank:\n",
    "            idx = scores[(i*attempts):((i+1)*attempts)].argmin()\n",
    "        else:\n",
    "            idx = 0\n",
    "        out_texts.append(results[i*attempts+idx])\n",
    "    return out_texts\n",
    "\n",
    "torch.manual_seed(0)\n",
    "paraphrase(['this whole article is bullshit!', 'bitch I warned yall. didnt i?', 'can someone please fix that crap', 'Its a very stupid name'], beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This article is a mess!',\n",
       " 'Bite I warned you.',\n",
       " \"I'd like someone to fix that.\",\n",
       " \"it's a very bad name.\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['this whole article is bullshit!', 'bitch I warned yall. didnt i?', 'can someone please fix that crap', 'Its a very stupid name'], beams=3, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was an apex.',\n",
       " 'Bite I warned you.',\n",
       " \"I'd like someone to fix that.\",\n",
       " \"it's a very bad name.\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['I was such an asshole', 'bitch I warned yall. didnt i?', 'can someone please fix that crap', 'Its a very stupid name'], beams=25, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"I'd like to thank you for that!\",\n",
       " \"it'll remind me of a duck.\",\n",
       " 'He has to be a TSA.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['I was such ana!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=10, rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceebc9366ccc43a6a548f01b8a5bfbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You'd be right. You'd be right. You'd\n",
      "a boss who's at the top of his game.\n",
      "A bad society does the wrong things, and votes for the wrong politicians.\n",
      "a president who's not.\n",
      "Colberg's a bad guy, and I'm a TSA.\n",
      "\"Studden,studden,studden\"\n",
      "I'd like to thank you, I think it's an error and the contributions are unwise.\n",
      "Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see\n",
      "Mormonis is the wrong thing to do, it's the wrong thing to do, it's the wrong thing to do, it's the wrong thing to do, it's the right thing to do, it's the right thing to do, it's the\n",
      "First of all, it'll be here. - It'll be here. - It'll be here. - It'll be here. - It'll be here. - It'll be there. - It'll be here.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cleanup()\n",
    "\n",
    "lines = test_data[:10]\n",
    "\n",
    "\n",
    "for i in trange(int(len(lines) / batch_size + 1)):\n",
    "    if i % 10 == 0:\n",
    "        cleanup()\n",
    "    t = i * batch_size\n",
    "    batch = [line.strip() for line in lines[t:(t+batch_size)]]\n",
    "    if not batch:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        res = paraphrase(batch, max_length='auto', beams=10)\n",
    "    except RuntimeError:\n",
    "        print('runtime error for batch ', i)\n",
    "        try:\n",
    "            cleanup()\n",
    "            res = [paraphrase([text], max_length='auto', beams=3)[0] for text in batch]\n",
    "        except RuntimeError:\n",
    "            print('runtime error for batch ', i, 'even with batch size 1')\n",
    "            res = batch\n",
    "            cleanup()\n",
    "    for out in res:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "You'd be right. You'd be right. You'd\n",
    "As snooty and overbearing as its\n",
    "A bad society does the wrong things, and votes for the wrong politicians.\n",
    "President, he's the man. He's the man.\n",
    "Colberg's a bad guy, and I'm a TSA.\n",
    "Dubious, dubious, dubious, dubious.\n",
    "I like you. I think you'll be an intelligent man and your contributions will be a big deal.\n",
    "Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see\n",
    "Mormonis is the wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The right thing to do. The right thing to do. The right thing to do \n",
    "You'd be a bad guy, uninitiated.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8bad1b2f7bb76525e63e863daf60af654c9de751e6d5c44e38cfc5e9e137cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
